{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a023c5f4",
   "metadata": {},
   "source": [
    "# Construction d'un pipeline de traitement de données issues des logs d'une application health en temps réel à l'intérieur de snowflake\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0dc721",
   "metadata": {},
   "source": [
    "## 1. Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481a9d6",
   "metadata": {},
   "source": [
    "**Création d'une database `health_app`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdfdf5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR ALTER DATABASE health_app;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a15676b",
   "metadata": {},
   "source": [
    "Sélectionne la base de données `\"health_app\"` comme base de travail par défaut.\n",
    "Toutes les opérations suivantes (création de schémas, tables, vues, tâches, etc.) seront exécutées dans ce contexte de base de données, sauf si un autre nom de base est explicitement précisé dans les instructions SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0f6f69",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "USE DATABASE health_app;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db13e8",
   "metadata": {},
   "source": [
    "**Création du schéma `raw`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e4a7e5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR ALTER SCHEMA raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea44e4",
   "metadata": {},
   "source": [
    "**Création des tables brutes pour recevoir les événements : `raw.raw_events`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137337b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR ALTER TABLE raw.raw_events (\n",
    "    event_timestamp TIMESTAMP,\n",
    "    process_name STRING,\n",
    "    process_id NUMBER,\n",
    "    message STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feb004",
   "metadata": {},
   "source": [
    "> À ce stade, le pipeline est prêt à recevoir des données mais rien n’est encore en mouvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce33a84",
   "metadata": {},
   "source": [
    "## 2. Ingestion automatique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1c0f7",
   "metadata": {},
   "source": [
    "**Définition du format de fichier (`raw.csv_file`) et du stage interne (`raw.internal_stage`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d96586",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Format CSV\n",
    "CREATE OR ALTER FILE FORMAT raw.csv_file \n",
    "TYPE = CSV \n",
    "FIELD_DELIMITER = '|' \n",
    "TIMESTAMP_FORMAT = 'YYYYMMDD-HH24:MI:SS:FF3';\n",
    "\n",
    "-- Stage interne\n",
    "CREATE OR ALTER STAGE raw.internal_stage\n",
    "FILE_FORMAT = raw.csv_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f226d9",
   "metadata": {},
   "source": [
    "**Création d’un pipe Snowpipe (`raw.load_raw_data`) configuré avec `AUTO_INGEST = TRUE`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ec107",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE PIPE raw.load_raw_data\n",
    "  AUTO_INGEST = TRUE\n",
    "  AS\n",
    "    COPY INTO RAW.RAW_EVENTS (event_timestamp, process_name, process_id, message)\n",
    "    FROM\n",
    "    (SELECT\n",
    "        $1 AS event_timestamp,\n",
    "        $2 AS process_name,\n",
    "        $3 AS process_id,\n",
    "        $4 AS message\n",
    "    FROM @raw.internal_stage)\n",
    "  FILE_FORMAT = raw.csv_file;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd29f75",
   "metadata": {},
   "source": [
    "> Dès qu’un fichier est déposé dans le stage, le pipe déclenche automatiquement le chargement vers raw.raw_events.\n",
    ">\n",
    "> Cela correspond à *l’entrée des données dans le pipeline*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa12ad",
   "metadata": {},
   "source": [
    "## 3. Stream sur la table brute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23270094",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE STREAM raw.raw_events_stream \n",
    "ON TABLE raw.raw_events \n",
    "APPEND_ONLY = TRUE;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed9344",
   "metadata": {},
   "source": [
    "> Le stream `raw.raw_events_stream` capture les changements incrémentaux (`INSERT` uniquement) sur `raw.raw_events`.\n",
    ">\n",
    "> Il agit comme un tampon : il permet aux tâches suivantes de traiter uniquement les nouvelles données arrivées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2160f40",
   "metadata": {},
   "source": [
    "## 4. Tables intermédiaires et de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0071f36b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Données à traiter\n",
    "CREATE OR ALTER TABLE raw.data_to_process (\n",
    "    event_id NUMBER,\n",
    "    event_timestamp TIMESTAMP,\n",
    "    process_name STRING,\n",
    "    process_id NUMBER,\n",
    "    message STRING\n",
    ");\n",
    "\n",
    "-- Table de log des traitements\n",
    "CREATE OR ALTER TABLE raw.logging (\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
    "    graph_run_group_id STRING,\n",
    "    table_name STRING,\n",
    "    n_rows NUMBER,\n",
    "    error_message STRING DEFAULT NULL\n",
    ");\n",
    "\n",
    "-- Table des anomalies\n",
    "CREATE OR ALTER TABLE raw.data_anomalies (\n",
    "    event_id NUMBER, \n",
    "    is_correct_timestamp BOOLEAN,\n",
    "    is_correct_process_name BOOLEAN,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),\n",
    "    graph_run_group_id STRING\n",
    ");\n",
    "\n",
    "-- Statut du pipeline\n",
    "CREATE OR ALTER TABLE raw.transformation_pipline_status (\n",
    "    graph_run_group_id STRING,\n",
    "    started_at TIMESTAMP,\n",
    "    finished_at TIMESTAMP,\n",
    "    status STRING\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae160ac4",
   "metadata": {},
   "source": [
    "- `raw.data_to_process` : staging des données à transformer.\n",
    "\n",
    "- `raw.logging` : suivi des logs d’exécution.\n",
    "\n",
    "- `raw.data_anomalies` : stockage des anomalies détectées.\n",
    "\n",
    "- `raw.transformation_pipline_status` : suivi global de l’état du pipeline (succès / échec).\n",
    "\n",
    "> Ces tables servent de support opérationnel et de métadonnées de suivi pour le pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbd6832",
   "metadata": {},
   "source": [
    "## 5. Fonctions UDF Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ac70d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE FUNCTION raw.extract_log_trigger(message STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "HANDLER = 'extract_log_trigger'\n",
    "AS $$\n",
    "def extract_log_trigger(message: str):\n",
    "    return message.strip().split(\" \")[0].split(\":\")[0].split(\"=\")[0].strip()\n",
    "$$;\n",
    "\n",
    "CREATE OR REPLACE FUNCTION raw.extract_log_message(message STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "RUNTIME_VERSION = '3.12'\n",
    "HANDLER = 'extract_log_trigger'\n",
    "AS $$\n",
    "def extract_log_trigger(message: str):\n",
    "    msg_trigger = message.strip().split(\" \")[0].split(\":\")[0].split(\"=\")[0].strip()\n",
    "    return message.replace(msg_trigger, \"\").strip()\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deedf2",
   "metadata": {},
   "source": [
    "`extract_log_trigger` et `extract_log_message` : fonctions de parsing du champ message.\n",
    "\n",
    "> Elles sont utilisées dans les procédures d’enrichissement pour séparer les éléments du log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e2cb3",
   "metadata": {},
   "source": [
    "## 6. Procédures stockées (logique métier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951b195",
   "metadata": {},
   "source": [
    "**L’ordre d’appel logique des procédures est le suivant :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183977d5",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- 6.1 Log des résultats\n",
    "CREATE OR REPLACE PROCEDURE raw.log_results(graph_run_group_id STRING, table_name STRING, n_rows NUMBER, error_message STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "INSERT INTO raw.logging (graph_run_group_id, table_name, n_rows, error_message)\n",
    "VALUES (:graph_run_group_id, :table_name, :n_rows, :error_message);\n",
    "$$;\n",
    "\n",
    "-- 6.2 Identification des nouvelles données\n",
    "CREATE OR REPLACE PROCEDURE raw.identify_new_data(graph_run_group_id STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "DECLARE\n",
    "    insert_exception EXCEPTION (-20001, 'Exception in data loading into raw.data_to_process table');\n",
    "BEGIN\n",
    "    LET n_rows INT := 0;\n",
    "    INSERT INTO raw.data_to_process (event_id, event_timestamp, process_name, process_id, message)\n",
    "    WITH source AS (\n",
    "        SELECT event_id, event_timestamp, process_name, process_id, message\n",
    "        FROM raw.raw_events_stream\n",
    "    )\n",
    "    SELECT * FROM source;\n",
    "    n_rows := SQLROWCOUNT;\n",
    "    CALL raw.log_results(:graph_run_group_id, 'raw.data_to_process', :n_rows, NULL);\n",
    "EXCEPTION\n",
    "    WHEN OTHER THEN\n",
    "        CALL raw.log_results(:graph_run_group_id, 'raw.data_to_process', NULL, :SQLERRM);\n",
    "        RAISE insert_exception;\n",
    "    RETURN :n_rows;\n",
    "END;\n",
    "$$;\n",
    "\n",
    "-- 6.3 Contrôle qualité\n",
    "CREATE OR REPLACE PROCEDURE raw.data_quality(graph_run_group_id STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "BEGIN\n",
    "    LET number_of_incorrect_lines INT := 0;\n",
    "    INSERT INTO raw.data_anomalies (event_id, is_correct_timestamp, is_correct_process_name, graph_run_group_id)\n",
    "    WITH source AS (\n",
    "        SELECT\n",
    "            event_id,\n",
    "            raw.check_correct_timestamp(event_timestamp) AS is_correct_timestamp,\n",
    "            raw.check_correct_process_name(process_name) AS is_correct_process_name\n",
    "        FROM raw.data_to_process\n",
    "    )\n",
    "    SELECT *, :graph_run_group_id\n",
    "    FROM source\n",
    "    WHERE is_correct_timestamp = FALSE OR is_correct_process_name = FALSE;\n",
    "    number_of_incorrect_lines := SQLROWCOUNT;\n",
    "    RETURN :number_of_incorrect_lines;\n",
    "END;\n",
    "$$;\n",
    "\n",
    "-- 6.4 Enrichissement des données\n",
    "CREATE OR REPLACE PROCEDURE raw.enrich_data(table_name STRING, process_name STRING, graph_run_group_id STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "DECLARE\n",
    "    full_table_name STRING := CONCAT('staging.', :table_name);\n",
    "    insert_exception EXCEPTION (-20002, 'Exception in data loading into staging tables');\n",
    "BEGIN\n",
    "    LET n_rows INT := 0;\n",
    "    INSERT INTO IDENTIFIER(:full_table_name) (event_timestamp, process_id, log_trigger, message)\n",
    "    WITH source AS (\n",
    "        SELECT\n",
    "            dtp.event_timestamp,\n",
    "            dtp.process_name,\n",
    "            dtp.process_id,\n",
    "            raw.extract_log_trigger(dtp.message) AS log_trigger,\n",
    "            raw.extract_log_message(dtp.message) AS message\n",
    "        FROM raw.data_to_process dtp\n",
    "        LEFT JOIN raw.data_anomalies da\n",
    "        ON dtp.event_id = da.event_id\n",
    "        WHERE process_name = :process_name \n",
    "        AND da.event_id IS NULL\n",
    "    )\n",
    "    SELECT event_timestamp, process_id, log_trigger, message\n",
    "    FROM source;\n",
    "    n_rows := SQLROWCOUNT;\n",
    "    CALL raw.log_results(:graph_run_group_id, :table_name, :n_rows, NULL);\n",
    "EXCEPTION\n",
    "    WHEN OTHER THEN\n",
    "        CALL raw.log_results(:graph_run_group_id, :table_name, NULL, :SQLERRM);\n",
    "        RAISE insert_exception;\n",
    "    RETURN :n_rows;\n",
    "END;\n",
    "$$;\n",
    "\n",
    "-- 6.5 Finalisation du pipeline\n",
    "CREATE OR REPLACE PROCEDURE raw.finalize_transformation(graph_run_group_id STRING, started_at STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "EXECUTE AS CALLER\n",
    "AS\n",
    "$$\n",
    "DECLARE\n",
    "    pipeline_exception EXCEPTION (-20003, 'Exception in the transformation pipeline');\n",
    "BEGIN\n",
    "    LET n_errors INT := 0;\n",
    "    SELECT COUNT(*) INTO n_errors\n",
    "    FROM raw.logging\n",
    "    WHERE graph_run_group_id = :graph_run_group_id AND error_message IS NOT NULL;\n",
    "    \n",
    "    INSERT INTO raw.transformation_pipline_status (graph_run_group_id, started_at, finished_at, status)\n",
    "    SELECT\n",
    "        :graph_run_group_id,\n",
    "        :started_at,\n",
    "        CURRENT_TIMESTAMP(),\n",
    "        IFF(:n_errors > 0, 'FAILED', 'SUCCEEDED');\n",
    "\n",
    "    IF (:n_errors = 0) THEN\n",
    "        TRUNCATE TABLE raw.data_to_process;\n",
    "    ELSE\n",
    "        RAISE pipeline_exception;\n",
    "    END IF;\n",
    "END;\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb294d",
   "metadata": {},
   "source": [
    "- `raw.identify_new_data`\n",
    "\n",
    "    - Charge les nouvelles lignes depuis le stream `raw.raw_events_stream` vers `raw.data_to_process`.\n",
    "\n",
    "    - Log le résultat.\n",
    "\n",
    "- `raw.data_quality`\n",
    "\n",
    "    - Contrôle la validité des données (timestamp, process_name).\n",
    "\n",
    "    - Alimente la table raw.data_anomalies.\n",
    "\n",
    "- `raw.enrich_data`\n",
    "\n",
    "    - Exécute le parsing (`extract_log_trigger`, `extract_log_message`) et insère les données enrichies dans les tables de `staging` correspondantes.\n",
    "\n",
    "    - Une instance par type de processus (ex. `Step_LSC`, `HiH_HiSyncUtil…`).\n",
    "\n",
    "- `raw.finalize_transformation`\n",
    "\n",
    "    - Vérifie si des erreurs sont survenues.\n",
    "\n",
    "    - Met à jour le statut du pipeline (`SUCCEEDED` ou `FAILED`).\n",
    "\n",
    "    - Vide la table intermédiaires `raw.data_to_process` si tout s’est bien passé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf8e56",
   "metadata": {},
   "source": [
    "## 7. Orchestration par les Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a5da3",
   "metadata": {},
   "source": [
    "Les tasks Snowflake définissent l’ordre d’exécution automatique :\n",
    "\n",
    "**Chaîne logique:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a87a6b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Racine : ingestion de nouvelles données\n",
    "CREATE OR ALTER TASK raw.identify_new_data_task\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "WHEN SYSTEM$STREAM_HAS_DATA('raw.raw_events_stream')\n",
    "AS\n",
    "DECLARE\n",
    "    graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN\n",
    "    CALL raw.identify_new_data(:graph_run_group_id);\n",
    "END;\n",
    "\n",
    "-- Contrôle qualité\n",
    "CREATE OR ALTER TASK raw.data_quality_task\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.identify_new_data_task\n",
    "AS\n",
    "DECLARE\n",
    "    graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN\n",
    "    CALL raw.data_quality(:graph_run_group_id);\n",
    "END;\n",
    "\n",
    "-- Enrichissements parallèles\n",
    "CREATE OR ALTER TASK raw.hih_listener_manager\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('hih_listener_manager', 'HiH_ListenerManager', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.hih_hibroadcastutil\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('hih_hi_broadcast_util', 'HiH_HiBroadcastUtil', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.step_standstepcounter\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('step_stand_step_counter', 'Step_StandStepCounter', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.step_sputils\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('step_sp_utils', 'Step_SPUtils', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.step_lsc\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('step_lsc', 'Step_LSC', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.hih_hihealthdatainsertstore\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('hih_hi_health_data_insert_store', 'HiH_HiHealthDataInsertStore', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.hih_datastatmanager\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('hih_data_stat_manager', 'HiH_DataStatManager', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.hih_hisyncutil\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('hih_hi_sync_util', 'HiH_HiSyncUtil', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.step_standreportreceiver\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('step_stand_report_receiver', 'Step_StandReportReceiver', :graph_run_group_id); END;\n",
    "\n",
    "CREATE OR ALTER TASK raw.step_screenutil\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "AFTER raw.data_quality_task\n",
    "AS\n",
    "DECLARE graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "BEGIN CALL raw.enrich_data('step_screen_util', 'Step_ScreenUtil', :graph_run_group_id); END;\n",
    "\n",
    "-- Task finale\n",
    "CREATE OR ALTER TASK raw.finalize_transformation_task\n",
    "WAREHOUSE = COMPUTE_WH\n",
    "FINALIZE = 'raw.identify_new_data_task'\n",
    "AS\n",
    "DECLARE\n",
    "    graph_run_group_id STRING := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_GROUP_ID');\n",
    "    started_at TIMESTAMP := SYSTEM$TASK_RUNTIME_INFO('CURRENT_TASK_GRAPH_RUN_ORIGINAL_SCHEDULED_TIMESTAMP');\n",
    "BEGIN\n",
    "    CALL raw.finalize_transformation(:graph_run_group_id, :started_at);\n",
    "END;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197a064",
   "metadata": {},
   "source": [
    "- `raw.identify_new_data_task` : déclencheur principal (s’exécute quand le stream a des données).\n",
    "\n",
    "- `raw.data_quality_task` : dépend du précédent (`AFTER identify_new_data_task`).\n",
    "\n",
    "- `Tasks d’enrichissement` : dépendent toutes de `data_quality_task`, et s’exécutent en parallèle.\n",
    "\n",
    "- `raw.finalize_transformation_task` : clôture le cycle (`FINALIZE`).\n",
    "\n",
    "> Cette orchestration assure un pipeline asynchrone, incrémental et résilient, entièrement géré dans Snowflake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa24bda",
   "metadata": {},
   "source": [
    "## 8. Activation du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8f2df",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "ALTER TASK raw.identify_new_data_task SUSPEND;\n",
    "\n",
    "ALTER TASK raw.data_quality_task RESUME;\n",
    "ALTER TASK raw.hih_listener_manager RESUME;\n",
    "ALTER TASK raw.hih_hibroadcastutil RESUME;\n",
    "ALTER TASK raw.step_standstepcounter RESUME;\n",
    "ALTER TASK raw.step_sputils RESUME;\n",
    "ALTER TASK raw.step_lsc RESUME;\n",
    "ALTER TASK raw.hih_hihealthdatainsertstore RESUME;\n",
    "ALTER TASK raw.hih_datastatmanager RESUME;\n",
    "ALTER TASK raw.hih_hisyncutil RESUME;\n",
    "ALTER TASK raw.step_standreportreceiver RESUME;\n",
    "ALTER TASK raw.step_screenutil RESUME;\n",
    "\n",
    "ALTER TASK raw.identify_new_data_task RESUME;\n",
    "ALTER TASK raw.finalize_transformation_task RESUME;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7202310a",
   "metadata": {},
   "source": [
    "> Suspension et reprise sélective des tasks pour s’assurer du bon ordre d’activation.\n",
    ">\n",
    "> Une fois toutes les tasks activées, le pipeline est entièrement opérationnel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e1cd2",
   "metadata": {},
   "source": [
    "## 9. Tests du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639df0a1",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "TRUNCATE TABLE raw.raw_events;\n",
    "TRUNCATE TABLE raw.data_to_process;\n",
    "TRUNCATE TABLE staging.step_lsc;\n",
    "\n",
    "INSERT INTO raw.raw_events (event_timestamp, process_name, process_id, message)\n",
    "VALUES ('2017-12-23 22:15:29.606'::TIMESTAMP, 'Step_LSC', 30002312, 'onStandStepChanged 3579');\n",
    "\n",
    "SELECT * FROM raw.raw_events_stream;\n",
    "\n",
    "SELECT *\n",
    "FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY(\n",
    "    SCHEDULED_TIME_RANGE_START=>DATEADD('hour',-1,current_timestamp())\n",
    "))\n",
    "WHERE schema_name = 'RAW';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e877d17",
   "metadata": {},
   "source": [
    "- Vidage des tables (`TRUNCATE`) pour un état propre.\n",
    "\n",
    "- Insertion d’un événement test.\n",
    "\n",
    "- Consultation du stream et de l’historique des tasks via `INFORMATION_SCHEMA.TASK_HISTORY`.\n",
    "\n",
    "> Permet de valider le déclenchement et le chaînage correct du pipeline."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
